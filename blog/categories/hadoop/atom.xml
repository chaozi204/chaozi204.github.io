<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | 超子博客]]></title>
  <link href="http://chaozi204.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://chaozi204.github.io/"/>
  <updated>2016-02-08T13:05:36+08:00</updated>
  <id>http://chaozi204.github.io/</id>
  <author>
    <name><![CDATA[超子]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop V2 Snappy 安装及部署]]></title>
    <link href="http://chaozi204.github.io/blog/2015/11/10/hadoop-v2-snappy-install/"/>
    <updated>2015-11-10T11:19:54+08:00</updated>
    <id>http://chaozi204.github.io/blog/2015/11/10/hadoop-v2-snappy-install</id>
    <content type="html"><![CDATA[<blockquote><p>最近同事有需求要使用在hadoop 2.5.1的集群上使用snappy压缩。因为之前没有使用过snappy，所以没有在现有的集群上配置相关信息，于是乎就在官方和google查了一下相关的配置信息。之所以写这篇文章，主要是因为网上发的文章（尤其是中国的某些网站）的内容不靠谱，没有仔细阅读文档，就随便写一些不经过实践的文章来忽悠大家，导致大家如果根据他们的方式来安装和使用，可能会导致莫名其妙的问题。闲话不说了，写下我的操作过程</p></blockquote>

<h5>1 安装 snappy lib库</h5>

<p>snappy是google的开源项目，目前代码已经迁移到github上，
地址为：<a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a> 。不过比较操蛋的事情是，git上的代码里面没有configure文件，导致我这种c菜鸟不会整了，于是只能在中国的不靠谱网站-csdn找到一个版本，比如我现在用的1.1.2版本。剩下的就是编译安装snappy库了，过程如下(前提你已经安装了相关的依赖库，这里不罗嗦)：</br></p>

<pre><code>./configure
make
make install
</code></pre>

<p>这样默认安装snappy库到/usr/local/lib下
 <!-- more --></p>

<h5>2 重新编译hadoop</h5>

<p>编译前建议仔细阅读Hadoop的BUILDING文件，使用如下命令进行编译</p>

<pre><code>mvn clean package -Pdist,native -DskipTests -Dtar  -Drequire.snappy  -Dbundle.snappy -Dsnappy.lib=/usr/local/lib
</code></pre>

<p>利用这个命令编译后的hadoop就会将snappy的native库拷贝到了hadoop/lib/native下面了，并且libhadoop的so文件中也会携带了相关的信息。</p>

<h5>3 部署</h5>

<ol>
<li>将新编译的hadoop native库下的内容替换线上的。</li>
<li>修改hadoop的配置属性，增加hadoop对snappy encode和decode类的配置
 <code>xml
 &lt;property&gt;
   &lt;name&gt;io.compression.codecs&lt;/name&gt;
   &lt;value&gt;
org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.BZip2Codec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec,
org.apache.hadoop.io.compress.SnappyCodec
  &lt;/value&gt;
&lt;/property&gt;
</code></li>
<li>重启集群</li>
<li>测试snappy。方法很简单，找一个snappy的文件，利用hadoop fs -text如果能打开则证明安装完成。或者使用hadoop的命令 hadoop checknative -a</li>
</ol>


<h5>4 解疑答惑</h5>

<ol>
<li><p>网上很多文章说还要安装hadoop-snappy，我想说的是，这个是针对hadoop v1的，因为hadoop v2中已经携带了snappy的decode和encode代码了。所以根本不需要安装这个jar，而且安装后使用hadoop checknative -a会报错，错误如下：
<code>java
Exception in thread "main" java.lang.NoSuchMethodError:
org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded()Z
     at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:82)
</code></p></li>
<li><p>如果snappy为安装好或安装成功，有可能出现的一种问题如下：</p></li>
</ol>


<pre><code class="java ">java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support.
        at org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:64)
        at org.apache.hadoop.io.compress.SnappyCodec.createDecompressor(SnappyCodec.java:201)
        at org.apache.hadoop.io.compress.SnappyCodec.createInputStream(SnappyCodec.java:161)
        at org.apache.hadoop.fs.shell.Display$Text.getInputStream(Display.java:150)
        at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:98)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:306)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:278)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:260)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:244)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Yarn 日志聚集权限警告]]></title>
    <link href="http://chaozi204.github.io/blog/2015/08/24/log-aggregation-permission-warning/"/>
    <updated>2015-08-24T18:04:00+08:00</updated>
    <id>http://chaozi204.github.io/blog/2015/08/24/log-aggregation-permission-warning</id>
    <content type="html"><![CDATA[<h3>问题描述</h3>

<p>集群中所有的nodemanager节点机器总是会报出警告信息</p>

<blockquote><p>警告信息：
2015-07-29 00:44:57,785 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService: Remote Root Log Dir [/yarn-logs] already exist, but with incorrect permissions. Expected: [rwxrwxrwt], Found: [rwxr-xr-x]. The cluster may have problems with multiple users.</p></blockquote>

<p>从警告的内容来看，似乎是目录的权限不匹配导致的，为了防止这个警告产生对集群的影响，于是排查本异常产生的原因。</p>

<!-- more -->


<h3>问题原因</h3>

<p>产生警告的类为 LogAggregationService.java , 其中产生警告的代码为
<code>java
 void verifyAndCreateRemoteLogDir(Configuration conf) {
  // Checking the existence of the TLD
  FileSystem remoteFS = null;
  try {
    remoteFS = getFileSystem(conf);
  } catch (IOException e) {
    throw new YarnRuntimeException("Unable to get Remote FileSystem instance", e);
  }
  boolean remoteExists = true;
  try {
    FsPermission perms =
        remoteFS.getFileStatus(this.remoteRootLogDir).getPermission();
    if (!perms.equals(TLDIR_PERMISSIONS)) {
      LOG.warn("Remote Root Log Dir [" + this.remoteRootLogDir
          + "] already exist, but with incorrect permissions. "
          + "Expected: [" + TLDIR_PERMISSIONS + "], Found: [" + perms
          + "]." + " The cluster may have problems with multiple users.");
    }
  } catch (FileNotFoundException e) {
    remoteExists = false;
  } catch (IOException e) {
    throw new YarnRuntimeException(
        "Failed to check permissions for dir ["
            + this.remoteRootLogDir + "]", e);
  }
  if (!remoteExists) {
    LOG.warn("Remote Root Log Dir [" + this.remoteRootLogDir
        + "] does not exist. Attempting to create it.");
    try {
      Path qualified =
          this.remoteRootLogDir.makeQualified(remoteFS.getUri(),
              remoteFS.getWorkingDirectory());
      remoteFS.mkdirs(qualified, new FsPermission(TLDIR_PERMISSIONS));
      remoteFS.setPermission(qualified, new FsPermission(TLDIR_PERMISSIONS));
    } catch (IOException e) {
      throw new YarnRuntimeException("Failed to create remoteLogDir ["
          + this.remoteRootLogDir + "]", e);
    }
  }
}
</code></p>

<p>代码中的第11-17行中就是这个警告产生的过程。从代码不难看出，这个警告不会对系统运行产生影响，也不会给系统造成多大的负担。
从代码的逻辑中，我们可以看出hadoop希望我们将聚合日志的目录（就是配置属性yarn.nodemanager.remote-app-log-dir指定的目录）设置为权限01777（其中1权限代码sticky bit），权限参考HDFS 权限说明 。而实际我们的集群因为保证完全的用户隔离和写安全，将日志聚合目录修改为了755权限（我们会单独为每个新用户创建相应的聚合目录），因此导致hadoop期望的权限和我们设置的权限不匹配，从而导致了问题的产生。</p>

<h3>解决方法</h3>

<p>解决方法1： 如果对系统的安全性要求没有特别的要求，可以完全将日志聚合目录的权限按hadoop的要求修改为01777权限，这个问题自然解决。</p>

<p>解决方法2：在创建新用户的情况下，可以手动给每个用户创建对应的聚合目录，也不会影响聚合功能的使用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Task读取HDFS数据导致任务失败或延迟]]></title>
    <link href="http://chaozi204.github.io/blog/2014/11/25/confession/"/>
    <updated>2014-11-25T23:00:37+08:00</updated>
    <id>http://chaozi204.github.io/blog/2014/11/25/confession</id>
    <content type="html"><![CDATA[<h3>问题描述</h3>

<ul>
<li><p>Hadoop Job在执行时非常缓慢（hadoop-1.0.0 和 hadoop-2.5.0集群中都有），且很多Map任务或reduce任务因为超时被kill掉，异常信息如下：
<code>Task attempt_201406261559_894052_m_000269_1 failed to report status for 602 seconds. Killing!</code></p></li>
<li><p>任务卡在初始化的过程中，也就是Map和Reduce的setup方法中
<code>protected void setup(Context context ) throws IOException, InterruptedException { }
</code></p></li>
<li>任务有些成功有些被kill，大部分情况下job最终执行是成功，只是比较耗时</li>
</ul>


<!-- more -->


<h3>问题分析及排查</h3>

<ul>
<li>首先排查GC导致的问题，检查其中几个任务的gc情况就可以得出是不是GC引起的</li>
<li>因为任务是因为超时被kill的，也就是说任务在10分钟左右都没有进度，且任务的状态是在RUNNING状态，也就是说任务要么卡在setup初始化中，要么是卡在map方法或reduce方法中比较耗时的操作</li>
<li>检查用户代码，主要查看setup方法和任务比较耗时的map或reduce方法</li>
<li><p>发现在setup方法中，会去读取hdfs文件，文件大小在100M左右
<code>java
public static String read(String filePath) throws IOException{  
  Configuration config = new Configuration();
  FSDataInputStream in = null;
  String fileName = filePath;
  StringBuffer rv=new StringBuffer();
  FileSystem hdfs = FileSystem.get(config);
  in = hdfs.open(new Path(fileName));
  String aline = null;
  BufferedReader bufread= new BufferedReader(new InputStreamReader(in, "UTF-8"));
  while((aline=bufread.readLine())!=null){
      rv.append(aline+"\n");}
  in.close();
  return rv.toString();
}
</code></p></li>
<li><p>此时开始怀疑是不是因为读取文件而导致这个问题呢。于是分析计算，总计2400 Map个，3个文件的备份，每个备份要负责800个任务的读取操作，假设网络带宽为100M跑满，也就是1秒中最多只能给1个map输送数据，这样全部完成需要800秒，超过了10分钟，也就是任务被kill的原因（以上分析完全是理想的情况下，实际情况更复杂），这是去查看cache的文件在机器的备份情况，然后通过ganglia查看机器当时的网络消费（的确是跑满了网络带宽，而且持续时间超过了10分钟），因此判断是因为网络堵塞导致任务失败.</p></li>
</ul>


<h3>解决方法</h3>

<ul>
<li>最好使用DistributeCache来代替直接读取hdfs文件的操作，这样不仅可以接受网络带宽，还能减少任务初始化的时间，减少因为本任务对其他任务的影响</li>
<li>次之方法是将文件的备份数增多，但是全部的网络消耗并没有节省，但是能够保证任务执行</li>
</ul>

]]></content>
  </entry>
  
</feed>
