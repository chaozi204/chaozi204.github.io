<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hive | 超子博客]]></title>
  <link href="http://chaozi204.github.io/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://chaozi204.github.io/"/>
  <updated>2016-02-04T17:05:48+08:00</updated>
  <id>http://chaozi204.github.io/</id>
  <author>
    <name><![CDATA[超子]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hive Udf 部署方式小结]]></title>
    <link href="http://chaozi204.github.io/blog/2015/11/06/hive-udf-deploy/"/>
    <updated>2015-11-06T18:37:17+08:00</updated>
    <id>http://chaozi204.github.io/blog/2015/11/06/hive-udf-deploy</id>
    <content type="html"><![CDATA[<h3>Hive Hook 部署方式：</h3>

<pre><code>本方式为我根据hive的hook的特性开发一种部署方式，优点是针对单个客户端，
无需修改HIVE的源码，纯外接的方式来完成udf的部署，比临时udfs方便快捷且永久，且容易管理。
每个组或每个团队之间的udf彼此不受影响。
相信安装部署见：https://github.com/chaozi204/hive-udf-hook
</code></pre>

<h3>临时UDF方法：</h3>

<pre><code>这个是最常见的Hive使用方式，通过hive的命令来完成UDF的部署
hive&gt; add jar xxxx.jar
hive&gt; CREATE TEMPORARY FUNCTION function_name AS 'udf类路径';
</code></pre>

<p>这种方法的的缺点很明显就是每次都需要使用这两个命令来完成操作</p>

<h3>永久部署UDF方法</h3>

<pre><code>这种方式是 hive 0.13版本以后开始支持的注册方法，详情可以参考官方说明。
使用方式下如：
CREATE FUNCTION [db_name.]function_name AS [USING JAR|FILE|ARCHIVE 'file_uri' [, JAR|FILE|ARCHIVE 'file_uri'] ];
例如：
CREATE FUNCTION udfDecoder AS  'com.sohu.tv.udf.udfDecoder' USING JAR 'hdfs://nn2.tvhadoop.sohuno.com/user/hive/udf/vr-hive-udf-0.0.1.jar'
</code></pre>

<p>这种方法的优点为全局可见，一次添加完成即可永久使用。支持数据库级别的函数名称。之所以能够永久性的部署，是因为hive将函数的数据存储到了数据库表FUNCS和FUNC_RU中。通过desc function function_name命令也可以知道这个函数是否是通过永久方式注册的</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用RCFile的特性提高压缩率]]></title>
    <link href="http://chaozi204.github.io/blog/2015/01/08/rcfile-assistant-compression/"/>
    <updated>2015-01-08T18:37:59+08:00</updated>
    <id>http://chaozi204.github.io/blog/2015/01/08/rcfile-assistant-compression</id>
    <content type="html"><![CDATA[<pre><code>最近公司集群的存储空间过于紧张，一度低于5% 。集群空间一下子成为了集群瓶颈。再申请扩容无望的情况下，我们不得不着手于通过业务节省空间，
或者强制进行删除文件。
同事无意中发现他的一份业务数据采用lzo + rcfile压缩后，压缩率超高，压缩前3G，压缩后200M。这种压缩率让我们感觉到异常，于是调查原因发现：
1. 日志相似性比较（这是业务本身的特性）
2. 生成结果数据是，利用李distribute by + sort by的hive特性，将相似的记录放在同一个reduce中，并根据特性字段排序
3. 利用rcfile的行列混合存储特性，就可以完成非常高的压缩率了
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive正则匹配中文问题]]></title>
    <link href="http://chaozi204.github.io/blog/2014/11/29/hive-chinese/"/>
    <updated>2014-11-29T17:34:37+08:00</updated>
    <id>http://chaozi204.github.io/blog/2014/11/29/hive-chinese</id>
    <content type="html"><![CDATA[<p>利用Hive的正则匹配中文时需要注意：</p>

<ul>
<li>中文的字符集合为[\u4e00-\u9fa5]</li>
<li>但是hive在hive执行中会被转义，因此需要增加一次java的转义字符才能够正确使用</li>
</ul>


<p>例如：
<code>select title from vid_title where type='my' and title rlike '^[\\\u4e00-\\\u9fa5]{1,2}$'</code></p>
]]></content>
  </entry>
  
</feed>
